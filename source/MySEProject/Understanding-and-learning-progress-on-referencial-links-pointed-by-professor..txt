
Learning Spatial Pooler and Boosting through the following sources

1. Boosting by HTM School youtube
	Learning Outcome: Able to Understand Boosting and How it is used. Boosting is nothing but to helps to change the overlap 
        score before the inhibition occurs giving less active columns a better chance to express themselves and diminishing columns that seem overactive .
 

2. Hierarchal Temporal Memory by medium.com webpost

Learning OutCome: 
        1. The Main Idea is to understand the basic of Artificial General Intelligence we need to decrypt the intelligence of NeoCortex
        Reverse Engineering of Neocortex consists of the following steps
            1.Sparse Distributed Representation
            2.Encoding
            3.Spatial Pooling
            4.Learning
            5.Boosting
            6.Temporal Pooling
	2. HTM starts with the assumption that everything the neocortex does is based on memory and recall of sequences of patterns.
        Sparse Distributed Representation:
        	SDRs are the brain's language. The word "Sparse" refers to the fact that just a few neurons fire when doing a cognitive activity, 
		i.e. if you take a picture of neurons in the brain, it is extremely likely that you will only observe less than 2% of neurons in an active state 
                (Experimental evidence of Sparse firing in the neocortex). An SDR is made up of thousands of bits, with a tiny fraction of the bits being 1's and 
                the remainder being 0's at any one moment. An SDR's bits correlate to neurons in the brain, with a 1 representing a moderately active neuron and 
                a 0 representing a comparatively dormant neuron.
        Encoding:
 		The encoder changes the data from its original format to an SDR that can be fed into an HTM system. 
                The encoder is in charge of identifying which output bits should be ones and which should be zeros 
                for a particular input value in order to capture the data's significant semantic properties. 
                SDRs with similar input values should have a high degree of overlap.
                Characteristics:
			-Semantically similar data should result in SDRs with overlapping active bits.
                        -Same input should always produce the same SDR as an output.
                        -The output should have the same dimensionality for all the inputs.
                        -The output should have similar sparsity for all inputs and have enough one bits to deal with noise and subsampling .
      
	Spatial Pooling:
		
             The spatial pooler takes the input data and converts it into active columns. 
	     Assume that for a given input space, a spatial pooling attempts to learn the sequences, 
             and that in order to learn the sequence, each micro column is linked to a particular number of synapses from the input. 
             The overlap score is then calculated, and if it is more than some permeance or threshold number, the column is activated; otherwise, it is not. 
	Learning:
		Learning takes place exclusively in the Spatial Pooler's active columns. 
                All connections that lie inside the input space will have their permeance value increased, 
                implying that the synaptic connection between them will be strengthened, whereas any connections 
                that fall outside of the input space will have their permeance value decremented. 
                Learning Spatial Pooler outperforms Random Spatial Pooler in terms of learning.
	Boosting:
		In order for a column in a Spatial pooler to exist, the overlap score must be greater than a certain threshold number, 
                whilst non-winning columns are prevented from learning. Only the winning columns have the ability to change their permanence values. 
                Boosting helps to improve the overlap score before inhibition occurs, providing less active columns a greater chance to express themselves and reducing hyperactive columns.
                Boosting on better allows for the learning of input data, which enhances efficiency. In other words, the columns with low overlap scores are boosted so that they may express themselves more effectively, 
                whereas the columns with high overlap scores are blocked because they express themselves too.
	Temporal Pooling:
		We can grasp the sequential pattern throughout time thanks to Temporal Pooling. 
                It learns the current column's sequences from the Spatial Pooler and guesses what spatial pattern will appear next depending on the temporal context of each input.
			

3. HTM Spatial Pooler by numenta youtube(Yuwei Cui)   	
4. Paper on BIOLOGICAL AND MACHINE INTELLIGENCE by numenta
	Learning OutCome:
	        The Imporatant Terminologies and there usage in spatial pooler.
				Column: An HTM region is organized in columns of cells. The SP operates at the column-level, where a
						column of a cells function as a single computational unit.
				Mini-column: See “Column”
				Inhibition: The mechanism for maintaining sparse activations of neurons. In the SP this manifests as
							columns inhibiting nearby columns from becoming active.
				Inhibition radius: The size of a column’s local neighborhood, within which columns may inhibit each
									other from becoming active.
 				Active duty cycle: A moving average denoting the frequency of column activation.
				Overlap duty cycle: A moving average denoting the frequency of the column’s overlap value being at
									least equal to the proximal segment activation threshold.
				Receptive field: The input space that a column can potentially connect to.
				Permanence value: indicates the amount of growth between a mini-column in the Spatial Pooling
									algorithm and one of the cells in its receptive field
				Permanence threshold: If a synapse’s permanence is above this value, it is considered fully connected.
									  Acceptable values are [0,1].
				Synapse: A junction between cells. In the Spatial Pooling algorithm, synapses on a column’s dendritic
						 segment connect to bits in the input space. A synapse can be in the following states:
						   Connected—permanence is above the threshold.
						   Potential—permanence is below the threshold.
						   Unconnected—does not have the ability to connect.
			Boosting:
				Boosting can be helpful in driving columns to compete for activation. Boosting is monitored by both the activity
				and overlap duty cycles (activeDutyCycle(c) and overlapDutyCycle(c), respectively). Following inhibition, if a
				column’s active duty cycle falls below the active duty cycles of neighboring columns, then its internal boost
				factor (boost(c)) will increase above one. If a column's active duty cycle arises above the active duty cycles of
				neighboring columns, its boost factor will decrease below one This helps drive the competition amongst
				columns and achieve the spatial pooling goal of using all the columns. Before inhibition, if a column’s overlap
				duty cycle is below its minimum acceptable value (calculated dynamically as a function of
				minPctOverlapDutyCycle and the overlap duty cycle of neighboring columns), then all its permanence values
				are boosted by the increment amount. A subpar duty cycle implies either a column's previously learned inputs
				are no longer ever active, or the vast majority of them have been "hijacked" by other columns. By raising all
				synapse permanences in response to a subpar duty cycle before inhibition, we enable a column to search for
				new inputs.
			From the above explination we can confirm that activeDutyCycle and overlapDutyCycle are some of the parameters that plays crucial role in Boosting.
			
			Imporatant parameters and their usage in spatial pooler and Boosting
			
				1. columnCount :The total number of columns in the Spatial Pooling algorithm, and the HTM
							 region. This is task dependent but we recommend a minimum value of 2048.
				2. overlap(c): The Spatial Pooling algorithm overlap of column c with a particular input
							pattern.
				3. activeColumns(t): List of column indices that are winners due to bottom-up input. 
				4. inhibitionRadius: Average connected receptive field size of the columns.
				5. boost(c): The boost value for column c as computed during learning – used to increase
						  the overlap value for inactive columns. 
				6. boostStrength: A number greater or equal than 0.0 to control the strength of boosting. No
								boosting is applied if boostStrength=0.
				7. synapse: A data structure representing a synapse, containing a permanence value and
                         the source input index. 
				8. activeDutyCycle(c): A sliding average representing how often column c has been active after
									inhibition (e.g. over the last 1000 iterations). 
				9. overlapDutyCycle(c): A sliding average representing how often column c has had significant overlap
									(i.e. greater than stimulusThreshold) with its inputs (e.g. over the last 1000
									 iterations). 
				10. updateActiveDutyCycle(c): Computes a moving average of how often column c has been active after
										  inhibition.
				11. updateOverlapDutyCycle(c): Computes a moving average of how often column c has overlap greater than
											stimulusThreshold. 
				12. maxDutyCycle(cols): Returns the maximum active duty cycle of the columns in the given list of
									columns.
				13. boostFunction(c): Returns the boost value of a column. The boost value is a positive scalar
								value. It is above one if the active duty cycle is above the mean active duty
								cycles of neighboring columns. It is less than one if a column has higher active
								duty cycle than its neighbors.

5.SpatialPooler.md - NeoCortexAPI Documentation from project
	Learning OutCome:
		Currently the project supports three versions of SP are implemented and considered:
			Spatial Pooler - single threaded original version without algorithm specific changes.
			SP-MT multithreaded version - which supports multiple cores on a single machine and
			SP-Parallel - which supports multicore and multimode calculus of spatial pooler.
		Steps to execute Spatial Pooler algorithm:
			1. Initialize Required Parameters using HtmConfig.
			2. Call Sp Compute Method to execute.
		The parameters used in HtmConfig are given Below
			POTENTIAL_RADIUS:	Defines the radius in number of input cells visible to column cells. It is important to choose this value, so every input neuron is connected to at least a single column. For example, if the input has 50000 bits and the column topology is 500, then you must choose some value larger than 50000/500 > 100.
			POTENTIAL_PCT:	Defines the percent of inputs withing potential radius, which can/should be connected to the column.
			GLOBAL_INHIBITION:	If TRUE global inhibition algorithm will be used. If FALSE local inhibition algorithm will be used.
			INHIBITION_RADIUS:	Defines neighbourhood radius of a column.
			LOCAL_AREA_DENSITY:	Density of active columns inside of local inhibition radius. If set on value < 0, explicit number of active columns (NUM_ACTIVE_COLUMNS_PER_INH_AREA) will be used.
			NUM_ACTIVE_COLUMNS_PER_INH_AREA:	An alternate way to control the density of the active columns. If this value is specified then LOCAL_AREA_DENSITY must be less than 0, and vice versa.
			STIMULUS_THRESHOLD:	One mini-column is active if its overlap exceeds overlap threshold  of connected synapses.
			SYN_PERM_INACTIVE_DEC:	Decrement step of synapse permanence value withing every inactive cycle. It defines how fast the NeoCortex will forget learned patterns.
			SYN_PERM_ACTIVE_INC:	Increment step of connected synapse during learning process
			SYN_PERM_CONNECTED:	Defines Connected Permanence Threshold  , which is a float value, which must be exceeded to declare synapse as connected.
			DUTY_CYCLE_PERIOD:	Number of iterations. The period used to calculate duty cycles. Higher values make it take longer to respond to changes in boost. Shorter values make it more unstable and likely to oscillate.
			MAX_BOOST:	Maximum boost factor of a column.
		SpatialPooler Algorithm working apporach :
			1. Starting with an SDR with randomly dispersed connections from each column to the input space, the process begins.
			A random persistence value between 0 and 1 is assigned to each link between an input bit and an output column. 
			If the connected persistence threshold p set in the parameter initialization is larger than the connected permanence threshold p, 
			the input bits connected to the given column are active. If a particular amount of input bits associated to that column are activated in the input space, 
			that column will be active. With these connections, Spatial Pooler will be able to represent various inputs as SDRs while still preserving the input's semantic content.
			The overlap score of the output columns is used to determine how comparable the information is. The more overlap there is between the two outputs, the closer they are.
		    2. Only the columns with the highest overlap score will be selected as active and permitted to learn, 
			while the remaining columns will remain unmodified. The active columns' connections that overlap the input will be strengthened by increasing the synaptic persistence value. 
			Meanwhile, the permanence value of other connections that do not match the input will be reduced.
		Boosting and Homeostatic plasticity controller:
			1.Normally, Spatial Pooler will only have a few active columns that represent distinct inputs, or their active duty-cycles will be near to one. 
			During the whole learning process, other inactive columns will never be active. This means that the output SDR can only explain a limited amount 
			of information about the input set. More columns will be able to participate in expressing the input space thanks to the boosting technique.
			2. The boosting approach of Spatial Pooler allows all columns to be used consistently across all patterns.
			Even though the columns have previously learnt patterns, the boosting process is still active, 
			causing the Spatial Pooler to forget the input. To address this issue, the Spatial Pooler now includes 
			a new homeostatic plasticity controller that turns off boosting once the learning has reached a stable state. 
			The output SDRs of the Spatial Pooler do not change over time, according to the research.
6. Improved HTM Spatial Pooler with Homeostatic Plasticity Control -Professor Dobric
	Learning OutCome:
		1. The Spatial Pooler works using mini-columns that are linked to sensory inputs . 
		   It is in charge of learning spatial patterns by encoding them into a sparse distributed representation (SDR). 
		   The SDR that was constructed to represent the encoded spatial pattern is then utilized as input for the Temporal Memory (TM) method.
		2. The TM is in charge of learning sequences from the SDR. The present version of the Spatial Pooler is instable, according to the results of this research. 
		   Learned patterns will be forgotten and re-learned during the learning process. The Spatial Pooler oscillates between stable and unstable stable, according on the results. 
		   Furthermore, investigations reveal that the instability is linked to a single pattern rather than a group of patterns.
		3. The majority of the trials were carried out using 2048 columns. The scalar encoder was utilized in this example to encode input scalar values supplied to the Spatial Pooler throughout the learning phase. 
		   Values ranging from 0 to 100 were utilized as input. Every input value was encoded with 200 bits before being presented to the Spatial Pooler, and each value is encoded with 15 non-zero bits.
		4. The Spatial Pooler method uses a column boosting technique inspired by homeostatic plasticity. 
		   This process affects the balance of excitation and inhibition in brain cells and is likely vital for maintaining the cortical state of stability. 
		   Homeostatic plasticity ensures the functional stability of neuronal networks. It balances network excitation and inhibition and coordinates circuit connection changes.
		5. The Spatial Pooler's boosting keeps track of column activity and ensures that all columns are used consistently throughout all patterns. 
		   Because this process is active all of the time, it can increase columns that have previously learnt SDRs. 
		   After then, the Spatial Pooler will "forget" certain learnt patterns for a short time. 
		   If the SP is shown the lost pattern again, it will begin to learn it again.
		6. The boosting was disabled by setting DUTY_CYCLE_PERIOD and MAX_BOOST to zero value. These two values disable boosting algorithm in the Spatial Pooler.
		   Results show that the SP with these parameters produces stable SDRs. The SP learns the pattern and encodes it to SDR in few iterations (typically 2-3) and keeps it unchanged
		   (stable) during the entire life cycle of the SP instance.By following this result, the stable SP can be achieved by disabling of the boosting algorithm.
		7. Unfortunately, without the boosting mechanism, the SP generates SDR-s with unpredictive number of active mini-columns.
		8. The further processing of memorized SDR-s will be badly effected if the number of active mini-columns in an SDR for distinct inputs is considerably variable. 
		   The computation of the overlap between brain cells, synapses, or mini-columns is used in most operations of Hierarchical Temporal Memory . In this situation, 
		   SDRs with a large number of active columns would statistically yield more overlaps than SDRs with fewer active cells, which is not balanced.
		9. The parameter NUM_ACTIVE_COLUMNS_PER_INH_AREA defines the percentage of columns in the inhibition area, which will be activated by the encoding of every single input pattern. Inspired by theneocortex, this value is typically set on 2% (Hawkins,
		   Subtei, 2016). By using the global inhibition in these experiments by the entire column set of 2048 columns the SP will generate SDRs with approx. 40 activecolumns.
		   The boosting mechanism inspired by homeostatic plasticity in neo-cortex solves this problem by consequent boosting of passive minicolumns\and inhibiting too active mini-columns.
		   As long the learning is occurring, the SP will continuously boost mini-columns. Every time the boosting takes a place, some learned patterns (SDRs) might be forgotten, and learning will continue when the same pattern appears the next time.
		10. It may be concluded that the boosting process has an impact on the SP's stability. The SP can reach a stable state,
			but SDRs with a drastically different number of active minicolumns will result. If boosting is turned on, the SP will activate mini-columns equitably, but the learning will be unstable.
	    11. Spatial can also benefit from the deactivation of the increase in homeostatic plasticity in the cortical layer L4. 
		    The specific workings of this system are currently unknown. However, by using what has been learned in this area,
			a comparable or identical process inside the SP may be implemented. This mechanism already exists in the HTM as boosting and inhibition algorithms that work on the mini-column level rather than the cell level inside the mini-column.
			SP does not use individual cells since it acts on the population of neural cells in mini-columns. Individual cells are more significant in the Temporal Memory algorithm than you would think.
		12. The fundamental concept behind this study is to add an extra algorithm to SP that does not interfere with the present SP method in order to stabilize the SP and maintain exploiting the plasticity. 
		    The Homeostatic Plasticity Controller, a new component, implements the method used by the expanded Spatial Pooler. The controller is "connected" to the Spatial Pooler's current implementation. 
		    The input pattern and related SDR are provided from the SP to the controller after each iteration's computation. The controller keeps the boosting going until the SP reaches a stable state, which is measured in repetitions. 
			During this period, the SP is in a stage known as "new-born" and will yield outcomes.
		13. When the SP reaches a stable state, the new algorithm turns off the boosting and informs the application of the change. 
		    The controller keeps track of how many mini-columns are present in the overall pattern. 
			The SP has entered the stable stage when the controller detects that all minicolumns are almost evenly employed and all visible SDRs are encoded with roughly the same amount of active mini-columns. 
			The SP will then exit the neonatal stage and resume normal operations, albeit without the boosting.

7. Started With running spatialpatternlearning experiment with no change in parameter values 
		OutCome:
			The SDR get stabilized at 97th Iteration.
8. Started Running SpatialPatternLearning experiment:
	case-1:
		Changed MaxBoost to 10 but didn't affcet the stablility it still taken 97th iteration to reach stability
	case-2:
		changed minOctOverlapCycles to 0 and maxBoost to 0 then it taken 112th iteration to reach stability
	Need to still explore why changing maxBoost alone didn't have affect on stability
	case-3:
		changed minOctOverlapCycles to 0 and maxBoost to 2 then it taken 91th iteration to reach stability
	so increasing Max_Boost results decreasing the iterations required to reach stability
	case-4:
		Changed minOctOverlapCycles to 5 and maxBoost to 5(unchanged)
		The Stability first reached at 97th iteration and again it eneters into INSTABLE STATE , then again it reaches stability at 258th iteration.
		Need to investigate why it becoming unstable in middle and again attains stability.
9. Analysis of UpdateBoostFactors method:
	1.updated maxBoost parameter value to 1,5 and 10 with DutyCyclePeriod parameter to 1000 for all the runs.
		The stability reached at cycle-96 and iteration 2 but again it becames unstable at cycle 181 and iteration 66. It again attained stable state at cycle-257 and iteration 93
		The same is observed for all the three case with maxBoost 1,5 and 10.
	
	2.But again ran the experiment with updating maxBoost parameter value to 1,5 and 10 with DutyCyclePeriod parameter to 150000 and IsBumpUpWeakColumnsDisabled to True for all the runs.
		case-1: MaxBoost=1 and DutyCyclePeriod=150000 and IsBumpUpWeakColumnsDisabled=True
			The stability reached at cycle-55 and iteration 0 but again it becames unstable at cycle 61 and iteration 18. It again attained stable state at cycle-111 and iteration 19 and 
			again it becames unstable at cycle 140 and iteration 99. It again attained stable state at cycle-191 and iteration 8 after that it didn't oscilated.
		For MaxBoost=5,7,10 and DutyCyclePeriod=150000 and IsBumpUpWeakColumnsDisabled=True
			The stability reached at cycle-111 and iteration 71 but again it becames unstable at cycle 135 and iteration 7. It again attained stable state at cycle-185 and iteration 8. after that it didn't oscilated.
			
		From the above analysis we observed that for maxboost=1 , we can attain stability quicky compared to 5 and 7 and 10, but it was going into INSTABLE state and again atatining stability



