Analyse and describe boosting algorithm.

1. Referred the content from medium.com which explains the fundamentals of the neocortex and the steps to involved in learning and experimenting the neocortex.

What's understood from this web post:

Vernon Mountcastle proposed a single powerful algorithm implemented by every region of cortex. If you connect regions of cortex together in a suitable hierarchy and provide a stream of input, it will learn about its environment. It's quiet inside. The brain learns only by learnings, it has no knowledge of itself. To explain it in a simple way lets say that, both auditory cortex and visual cortex are unified by one algorithm. The surprising result is that the ferrets develop functioning visual pathways in the auditory portions of their brains. In other words, they see with brain tissue that normally hears sounds i.e the cortex does something universal that can be applied to any type of sensory or motor system.

Neocortex:
It is like a sheet of napkin squeezed into a skull just below the dura matter of the skull consisting of all the areas required for cognition. To understand the basis of artificial general intelligence we need to decipher the intelligence of neocortex.

There are 6 steps involved in reverse engineering the neocortex:

* Sparse Distributed Representation
* Encoding
* Spatial Pooling
* Learning
* Boosting
* Temporal Pooling

HTM - Hierarchical temporal memory.

HTM starts with the assumption that everything the neocortex does is based on memory and recall of sequences of patterns.

The basic principle of HTM (Hierarchical Temporal Memory)/CLA(Cortical Learning Algorithm) is Temporal Memory i.e the memory of sequences. This theory postulates that every excitatory neuron in the neocortex is learning transitions of patterns and that the majority of synapses on every neuron are dedicated to learning these transitions.



1.Sparse Distributed Representations:

SDRs are the language of brain. The reason why it is termed as Sparse is because of the fact that only few neurons fire when a cognitive task is performed i.e. if you take a snapshot of neurons in the brain,   it is highly likely that you will only see less than 2% neurons in an active state (Experimental evidence of Sparse firing in the neocortex). An SDR consists of thousands of bits where at any point in time a    small percentage of the bits are 1's and the rest are 0's. The bits in an SDR correspond to neurons in the brain, a 1 being a relatively active neuron and a 0 being a relatively inactive neuron.

There are certain properties of SDRs:
* Capacity of SDRs and the probability of mismatches
* Robustness of SDRs and the probability of error with noise
* Reliable classification of a list of SDR vectors
* Unions of SDRs
* Robustness of unions in the presence of noise


2.Encoding:

- The encoder converts the native format of the data into an SDR that can be fed into an HTM system. 
- The encoder is in charge of identifying which output bits should be ones and which should be zeros for a particular input value in order to capture the data's significant semantic properties.
- SDRs with similar input values should have a high degree of overlap.

Characteristics:
- Semantically similar data should result in SDRs with overlapping active bits.
- Same input should always produce the same SDR as an output.
- The output should have the same dimensionality for all the inputs.
- The output should have similar sparsity for all inputs and have enough one bits to deal         with noise and subsampling .

3. Spacial pooling:

Spatial pooling Algorithm is for solving problems to represent the input active neurons (comping from sensory or motor organs) to make that the cortex learn the pattern of sequences. It accepts the input   vector of different sizes and represents it into sparse vectors of same size(It kind of normalises it). The output of the Spatial Pooler represents the mini columns i.e the pyramidal neuron in the cortices.It possess certain properties like maintaining a fixed sparsity i.e no matter how many bits are on and off in the input the spatial pooler need to maintain the fixed sparsity and to maintain the overlap   properties i.e two similar inputs should produce similar outputs in the columns. The spatial pooler takes the input data and translates the incoming data into active columns.


4. Learning:

- Learning takes place exclusively in the Spatial Pooler's active columns. 
- All connections that lie inside the input space will have their permeance value increased,implying that the synaptic connection between them will be strengthened, whereas any connections that fall outside of the input space will have their permeance value decremented. 
- Learning Spatial Pooler outperforms Random Spatial Pooler in terms of learning.

5. Boosting:

In order for a column in a Spatial pooler to exist it should be a winning column i.e the overlap score should be above some threshold value while non-winning columns are inhibited from learning. Only the    winner columns can update their permanence values. Boosting helps to change the overlap score before the inhibition occurs giving less active columns a better chance to express themselves and diminishing    columns that seem overactive

6. Temporal pooling:

Temporal Pooling enables us to understand the sequential pattern over time. It learns the sequences of the active column from the Spatial Pooler and predicts what spatial pattern in coming next based on the temporal context of each input .

#What is SDR?

The neocortex which is the seat of intelligent thought in the mammalian brain. High level vision, hearing, touch, movement, language, and planning are all performed by the neocortex. The activity of the neurons in the neocortex is sparse, meaning only a small percentage of neurons are spiking at any point in time. The sparsity might vary from less than one percent to several percent but is always sparse.

Hierarchical Temporal Memory (HTM) is a machine learning technology that aims to capture the structural and algorithmic properties of the neocortex. HTM systems require data in the form of Sparse Distributed Representations. “Sparse” means that only a small percentage of neurons are active at one time. “Distributed” means that the activations of many neurons are required in order to represent something. A single active neuron conveys some meaning but it must be interpreted within the context of a population of neurons to convey the full meaning.

Sparse Distributed Representations (SDRs) are binary representation i.e. an array consisting of large number of bits where small percentage are 1’s represents an active neuron and 0 an inactive one. Each bit typically has some meaning (such as the presence of an edge at a particular location and orientation). This means that if two vectors have 1s in the same position they are semantically similar in that attribute.

The first step of using an HTM system is to convert a data source into an SDR using an encoder. The encoder converts the native format of the data into an SDR that can be fed into an HTM system. The encoder is responsible for determining which output bits should be ones, and which should be zeros, for a given input value in such a way as to capture the important semantic characteristics of the data. Similar input values should produce highly overlapping SDRs.


# Boosting and important parameters:

			Boosting:
				Boosting can be helpful in driving columns to compete for activation. Boosting is monitored by both the activity
				and overlap duty cycles (activeDutyCycle(c) and overlapDutyCycle(c), respectively). Following inhibition, if a
				column’s active duty cycle falls below the active duty cycles of neighboring columns, then its internal boost
				factor (boost(c)) will increase above one. If a column's active duty cycle arises above the active duty cycles of
				neighboring columns, its boost factor will decrease below one This helps drive the competition amongst
				columns and achieve the spatial pooling goal of using all the columns. Before inhibition, if a column’s overlap
				duty cycle is below its minimum acceptable value (calculated dynamically as a function of
				minPctOverlapDutyCycle and the overlap duty cycle of neighboring columns), then all its permanence values
				are boosted by the increment amount. A subpar duty cycle implies either a column's previously learned inputs
				are no longer ever active, or the vast majority of them have been "hijacked" by other columns. By raising all
				synapse permanences in response to a subpar duty cycle before inhibition, we enable a column to search for
				new inputs.
			From the above explination we can confirm that activeDutyCycle and overlapDutyCycle are some of the parameters that plays crucial role in Boosting.
			
			Imporatant parameters and their usage in spatial pooler and Boosting
			
				1. columnCount :The total number of columns in the Spatial Pooling algorithm, and the HTM
							 region. This is task dependent but we recommend a minimum value of 2048.
				2. overlap(c): The Spatial Pooling algorithm overlap of column c with a particular input
							pattern.
				3. activeColumns(t): List of column indices that are winners due to bottom-up input. 
				4. inhibitionRadius: Average connected receptive field size of the columns.
				5. boost(c): The boost value for column c as computed during learning – used to increase
						  the overlap value for inactive columns. 
				6. boostStrength: A number greater or equal than 0.0 to control the strength of boosting. No
								boosting is applied if boostStrength=0.
				7. synapse: A data structure representing a synapse, containing a permanence value and
                         the source input index. 
				8. activeDutyCycle(c): A sliding average representing how often column c has been active after
									inhibition (e.g. over the last 1000 iterations). 
				9. overlapDutyCycle(c): A sliding average representing how often column c has had significant overlap
									(i.e. greater than stimulusThreshold) with its inputs (e.g. over the last 1000
									 iterations). 
				10. updateActiveDutyCycle(c): Computes a moving average of how often column c has been active after
										  inhibition.
				11. updateOverlapDutyCycle(c): Computes a moving average of how often column c has overlap greater than
											stimulusThreshold. 
				12. maxDutyCycle(cols): Returns the maximum active duty cycle of the columns in the given list of
									columns.
				13. boostFunction(c): Returns the boost value of a column. The boost value is a positive scalar
								value. It is above one if the active duty cycle is above the mean active duty
								cycles of neighboring columns. It is less than one if a column has higher active
								duty cycle than its neighbors.



# SpatialPooler.md - NeoCortexAPI Documentation from project
	                Learning OutCome:
		        Currently the project supports three versions of SP are implemented and considered:
			Spatial Pooler - single threaded original version without algorithm specific changes.
			SP-MT multithreaded version - which supports multiple cores on a single machine and
			SP-Parallel - which supports multicore and multimode calculus of spatial pooler.

		        Steps to execute Spatial Pooler algorithm:

			1. Initialize Required Parameters using HtmConfig.
			2. Call Sp Compute Method to execute.

		The parameters used in HtmConfig are given Below

			POTENTIAL_RADIUS:Defines the radius in number of input cells visible to column cells. It is important to choose this value, so every input neuron is connected to at least a single               column. For example, if the input has 50000 bits and the column topology is 500, then you must choose some value larger than 50000/500 > 100.
			POTENTIAL_PCT:	Defines the percent of inputs withing potential radius, which can/should be connected to the column.
			GLOBAL_INHIBITION:If TRUE global inhibition algorithm will be used. If FALSE local inhibition algorithm will be used.
			INHIBITION_RADIUS:Defines neighbourhood radius of a column.
			LOCAL_AREA_DENSITY:Density of active columns inside of local inhibition radius. If set on value < 0, explicit number of active columns (NUM_ACTIVE_COLUMNS_PER_INH_AREA) will be used.
			NUM_ACTIVE_COLUMNS_PER_INH_AREA:	An alternate way to control the density of the active columns. If this value is specified then LOCAL_AREA_DENSITY must be less than 0, and vice versa.
			STIMULUS_THRESHOLD:One mini-column is active if its overlap exceeds overlap threshold  of connected synapses.
			SYN_PERM_INACTIVE_DEC:Decrement step of synapse permanence value withing every inactive cycle. It defines how fast the NeoCortex will forget learned patterns.
			SYN_PERM_ACTIVE_INC:Increment step of connected synapse during learning process
			SYN_PERM_CONNECTED:Defines Connected Permanence Threshold  , which is a float value, which must be exceeded to declare synapse as connected.
			DUTY_CYCLE_PERIOD:Number of iterations. The period used to calculate duty cycles. Higher values make it take longer to respond to changes in boost. Shorter values make it more unstable and likely to oscillate.
			MAX_BOOST:Maximum boost factor of a column.

		SpatialPooler Algorithm working apporach :

			1. Starting with an SDR with randomly dispersed connections from each column to the input space, the process begins.
			A random persistence value between 0 and 1 is assigned to each link between an input bit and an output column. 
			If the connected persistence threshold p set in the parameter initialization is larger than the connected permanence threshold p, 
			the input bits connected to the given column are active. If a particular amount of input bits associated to that column are activated in the input space, 
			that column will be active. With these connections, Spatial Pooler will be able to represent various inputs as SDRs while still preserving the input's semantic content.
			The overlap score of the output columns is used to determine how comparable the information is. The more overlap there is between the two outputs, the closer they are.
		        2. Only the columns with the highest overlap score will be selected as active and permitted to learn, 
			while the remaining columns will remain unmodified. The active columns' connections that overlap the input will be strengthened by increasing the synaptic persistence value. 
			Meanwhile, the permanence value of other connections that do not match the input will be reduced.

		 Boosting and Homeostatic plasticity controller:

			1.Normally, Spatial Pooler will only have a few active columns that represent distinct inputs, or their active duty-cycles will be near to one. 
			During the whole learning process, other inactive columns will never be active. This means that the output SDR can only explain a limited amount 
			of information about the input set. More columns will be able to participate in expressing the input space thanks to the boosting technique.
			2. The boosting approach of Spatial Pooler allows all columns to be used consistently across all patterns.
			Even though the columns have previously learnt patterns, the boosting process is still active, 
			causing the Spatial Pooler to forget the input. To address this issue, the Spatial Pooler now includes 
			a new homeostatic plasticity controller that turns off boosting once the learning has reached a stable state. 
			The output SDRs of the Spatial Pooler do not change over time, according to the research.